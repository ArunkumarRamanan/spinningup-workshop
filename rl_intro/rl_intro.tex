\documentclass[9pt]{beamer}
\usepackage{beamerthemesplit}
%\usepackage{times}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{thmtools,thm-restate}
\usepackage[font=small]{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{animate}
\usepackage{media9}
\usepackage{multimedia}
\usepackage{hyperref}
\usepackage{cancel}

\usetheme{Madrid}

\graphicspath{{./images/}}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}


\newcommand{\underE}[2]{\underset{\begin{subarray}{c}#1 \end{subarray}}{\E}\left[ #2 \right]}


\newcommand\Fontvi{\fontsize{6}{7.2}\selectfont}
\renewcommand{\d}[1]{\ensuremath{\operatorname{d}\!{#1}}}

\newcommand{\twocolumns}[4]{
\begin{columns}
\begin{column}{#1\textwidth}
    #3
\end{column}
\begin{column}{#2\textwidth}
	#4
\end{column}
\end{columns}
}

\usepackage{listings}
\usepackage{color}
\usepackage{courier}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}


\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize\ttfamily,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    %numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstdefinestyle{mystyle2}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\tiny\ttfamily,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    %numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstdefinestyle{mystyle3}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\scriptsize\ttfamily,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    %numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle3}


\begin{document}
\input{exercises_def.tex}

\title{Introduction to Deep RL, Part 1}
\author{Joshua Achiam}
\institute{OpenAI}
\titlegraphic{\includegraphics[height=2cm]{spinning-up-logo2}}
%\date{March 3, 2018}

\begin{frame}
\titlepage
\end{frame}


\section{1.0: What is Spinning Up?}

\input{1-0_spinning_up.tex}

\section{1.1: What is deep RL, and why do we need it?}

\input{1-1_why_deep_rl.tex}

\section{Interlude: Recap of deep learning patterns}

\input{1-1-5_deep_learning.tex}

\section{1.2: How do we formulate RL problems?}

\input{1-2_formulate_rl.tex}

\section{1.3: What kinds of RL algorithms are there?}

\begin{frame}{Deep RL Algorithms}
There are many different kinds of RL algorithms! This is a non-exhaustive taxonomy (with specific algorithms in blue):
\begin{figure}
\centering
\includegraphics[width=\linewidth]{rl_algorithms_9_15}
\end{figure}
\end{frame}

\begin{frame}{Alphabet soup aside---what are they doing?}


\begin{columns}
\begin{column}{0.33\textwidth}
\includegraphics[width=0.9\textwidth]{rl_loop}
\end{column}
\begin{column}{0.67\textwidth}
Key steps in all RL algorithms:
\begin{itemize}
\item Run policy: actually act in env
\item Evaluate policy: estimate $V^{\pi}$ or $Q^*$
\item Improve policy: do something which lets you pick better actions
\end{itemize}

Major design decisions:
\begin{itemize}
\item Use a model of the env or not (can slot into any of those three steps)
\item Optimize stochastic policy directly or learn $Q^*$ as main controller
\end{itemize}
\end{column}
\end{columns}

\end{frame}

\begin{frame}{Model-Free RL: Policy Optimization}
\begin{figure}
\centering
\includegraphics[width=\linewidth]{rl_algorithms_polopt_only}
\end{figure}
\end{frame}

\begin{frame}{Model-Free RL: Policy Optimization}


\begin{columns}
\begin{column}{0.33\textwidth}
\includegraphics[width=0.9\textwidth]{rl_loop}
\end{column}
\begin{column}{0.67\textwidth}
\textbf{Policy Optimization}
\begin{itemize}
\item Run policy: Collect trajectories $\tau \sim \pi_{\theta}$
\item Evaluate policy: Estimate $V^{\pi_{\theta}}$, $A^{\pi_{\theta}}$ using current trajectories (\textbf{on-policy})
\item Improve policy: Increase likelihood of actions with high advantage
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Warning: Math}

\begin{figure}
\centering
\includegraphics[width=4cm]{palpatine}
\end{figure}

\end{frame}

\begin{frame}{Wait, but why?}


\textbf{Deep RL is not mature enough to be a black box yet: you need to know some of this stuff to work with it successfully.}

\end{frame}

\begin{frame}{Mathematical Foundations of Policy Optimization}

In policy optimization, we train a \textbf{stochastic policy} $\pi_{\theta}$, usually in an \textbf{on-policy way}, to \textbf{maximize performance} $J(\pi_{\theta})$.

\vspace{1em}

What we'll cover:
\begin{itemize}
\item Direct gradient ascent on $J(\pi_{\theta})$ (VPG)
\item Optimization of surrogate advantage functions (TRPO and PPO)
\end{itemize}

\end{frame}

\begin{frame}{The Policy Gradient}

Goal: derive an expression for $\nabla_{\theta} J(\pi_{\theta})$ which we can compute with a sample estimate, as the basis for a direct gradient ascent algorithm
%
\begin{equation*}
\theta \leftarrow \theta + \alpha \nabla_{\theta} J(\pi_{\theta})
\end{equation*}

\pause
\vspace{1em}

Well what happens if we just...
%
\begin{align*}
\nabla_{\theta} J(\pi_{\theta}) &= \nabla_{\theta} \underE{\tau \sim \pi_{\theta}}{R(\tau)}
\end{align*}

Problem: parameters are in distribution!

\end{frame}

\begin{frame}{The Policy Gradient: First Steps}

Solution: expand expectation into integral, use log-derivative trick
%
\begin{align*}
\nabla_{\theta} J(\pi_{\theta}) &= \nabla_{\theta} \underE{\tau \sim \pi_{\theta}}{R(\tau)} \\
&= \nabla_{\theta} \int d\tau P(\tau | \pi_{\theta}) R(\tau) \\
&= \int d\tau \nabla_{\theta} P(\tau | \pi_{\theta}) R(\tau) \\
&= \int d\tau {\color{red} P(\tau | \pi_{\theta}) \nabla_{\theta} \log P(\tau | \pi_{\theta})} R(\tau) \\
&= \underE{\tau \sim \pi_{\theta}}{\nabla_{\theta} \log P(\tau | \pi_{\theta}) R(\tau) }
\end{align*}

But are we done yet? No! Still need to compute $\nabla_{\theta} \log P(\tau | \pi_{\theta})$

\end{frame}

\begin{frame}{The Policy Gradient: Gradient of Trajectory Distribution}

What is $P(\tau|\pi_{\theta})$?
%
\begin{equation*}
P(\tau | \pi_{\theta}) = \mu(s_0) \prod_{t=0}^T P(s_{t+1} | s_t, a_t) \pi_{\theta}(a_t |s_t)
\end{equation*}

Thus:
%
\begin{align*}
\nabla_{\theta} \log P(\tau|\pi_{\theta}) &= \nabla_{\theta} \log \left(\mu(s_0) \prod_{t=0}^T P(s_{t+1} | s_t, a_t) \pi_{\theta}(a_t |s_t)\right) \\
&= \nabla_{\theta} \left(\log \mu(s_0) + \sum_{t=0}^T \left(\log P(s_{t+1} | s_t, a_t) + \log \pi_{\theta}(a_t |s_t) \right)\right) \\
&= \nabla_{\theta} \log \mu(s_0) + \sum_{t=0}^T \left(\nabla_{\theta} \log P(s_{t+1} | s_t, a_t) + \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \right) \\
&= {\color{red}\cancel{\nabla_{\theta} \log \mu(s_0)}} + \sum_{t=0}^T \left({\color{red}\cancel{\nabla_{\theta} \log P(s_{t+1} | s_t, a_t)}} + \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \right) \\
\therefore \nabla_{\theta} \log P(\tau|\pi_{\theta}) &= \sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t)
\end{align*}

\end{frame}

\begin{frame}{The Policy Gradient: So Far}
Putting it all together so far:
%
\begin{equation*}
\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) R(\tau)}
\end{equation*}

So we could estimate with:
%
\begin{align*}
\nabla_{\theta} J(\pi_{\theta}) &\approx \frac{1}{|\calD|} \sum_{\tau \in\calD} \sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) R(\tau)
\end{align*}

But not good enough! Variance will be high.

\end{frame}

\begin{frame}{The Policy Gradient: Variance Reduction}

Insight: future actions and past rewards should be uncorrelated. That is:
%
\begin{equation*}
\text{for } t > t', \;\;\; \E\left[\nabla_{\theta} \log \pi_{\theta}(a_t | s_t) r_{t'}\right] = 0
\end{equation*}

Thus:
%
\begin{align*}
\nabla_{\theta} J(\pi_{\theta}) &= \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) R(\tau)} \\
&= \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \sum_{t'=0}^T r_{t'}} \\
&= \sum_{t=0}^T \sum_{t'=0}^T \underE{\tau \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t | s_t)r_{t'}} \\
&= \sum_{t=0}^T \sum_{{\color{red}t'=t}}^T\underE{\tau \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t | s_t)r_{t'}} \\
\therefore \nabla_{\theta} J(\pi_{\theta}) &= \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \sum_{t'=t}^T r_{t'}} 
\end{align*}

\end{frame}

\begin{frame}{The Policy Gradient: Alternate Forms}

What we have currently: ``Reward-to-Go" policy gradient:
%
\begin{equation*}
\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \sum_{t'=t}^T r_{t'}} 
\end{equation*}

Observe: expectation can be broken up, letting us transform ``Reward-to-Go" into $Q^{\pi}$:
%
\begin{align*}
\nabla_{\theta} J(\pi_{\theta}) &= \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \sum_{t'=t}^T r_{t'}} \\
&= \sum_{t=0}^T \underE{\tau \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \sum_{t'=t}^T r_{t'}} \\
&= \sum_{t=0}^T \underE{\tau_{0:t} \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \underE{\tau_{(t+1):T} \sim \pi_{\theta}}{\sum_{t'=t}^T r_{t'}}} \\
&= \sum_{t=0}^T \underE{\tau_{0:t} \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t | s_t) Q^{\pi_{\theta}}(s_t, a_t)} \\
\therefore \nabla_{\theta} J(\pi_{\theta}) &=\underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) Q^{\pi_{\theta}}(s_t, a_t)}
\end{align*}

\end{frame}

\begin{frame}{The Policy Gradient: Baselines}

What is a \textbf{baseline}? A function $b(s_t)$ with
%
\begin{equation*}
\nabla_{\theta} J(\pi_{\theta}) =\underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \left(Q^{\pi_{\theta}}(s_t, a_t) - b(s_t)\right)}
\end{equation*}
%
Claim: this works for any $b$! Proof:
%
\begin{align*}
\underE{a_t \sim \pi_{\theta}(\cdot|s_t)}{\nabla_{\theta} \log \pi_{\theta}(a_t | s_t) b(s_t)} &= \underE{a_t \sim \pi_{\theta}(\cdot|s_t)}{\nabla_{\theta} \log \pi_{\theta}(a_t | s_t)} b(s_t) \\
&= \left(\int da \pi_{\theta}(a_t|s_t) \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)\right) b(s_t) \\
&=  \left(\int da {\color{red} \nabla_{\theta} \pi_{\theta}(a_t|s_t) }\right) b(s_t) \\
&= \left({\color{red} \nabla_{\theta}}\int da \pi_{\theta}(a_t|s_t) \right) b(s_t) \\
&=\cancel{\left(\nabla_{\theta} 1 \right)} b(s_t) \\
&= 0
\end{align*}

\end{frame}

\begin{frame}{The Policy Gradient: Advantage Form}
If we choose $b = V^{\pi}$, we get the \textbf{advantage form} of the policy gradient:
%
\begin{align*}
\nabla_{\theta} J(\pi_{\theta}) &=\underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \left(Q^{\pi_{\theta}}(s_t, a_t) - V^{\pi_{\theta}}(s_t)\right)}\\
 &=\underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) A^{\pi_{\theta}}(s_t, a_t)}
\end{align*}

Why do we want this? Better signal in sample estimate: removes ``stuff that would have happened anyway" from $Q^{\pi}$
\end{frame}

\begin{frame}{The Policy Gradient: Review}

What we've shown so far:
%
\begin{align*}
\nabla_{\theta} J(\pi_{\theta}) &=\underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) R(\tau)}\\ 
&=\underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \sum_{t'=t}^T r_{t'}}\\
&=\underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) Q^{\pi_{\theta}}(s_t, a_t)}\\
 &=\underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) A^{\pi_{\theta}}(s_t, a_t)}
\end{align*}

Which form do we use? Almost always the last one.

\end{frame}

\begin{frame}{The Policy Gradient: Policy Evaluation}

The policy gradient expression gives us the \textbf{policy improvement} step, but we need to compute advantages: so how do we do \textbf{policy evaluation}?

\vspace{1em}

Answer: first learn $V^{\pi}$ approximator by regression:
%
\begin{equation*}
\min_{\phi} \frac{1}{N}\sum_{\tau \in \calD} \sum_{t=0}^T \left(V_{\phi}(s_t) - \sum_{t'=t}^T \gamma^{t'-t} r_{t'}\right)^2
\end{equation*}

Then use it to estimate $A^{\pi}$, usually with \textbf{generalized advantage estimation}

\end{frame}

\begin{frame}{The Policy Gradient: Generalized Advantage Estimation}

N-Step Advantage Estimates:
%
\begin{equation*}
\hat{A}_t^{\pi(n)} = \underbrace{\sum_{t'=t}^n \gamma^{t'-t} r_{t'} + \gamma^{n+1} V_{\phi}(s_{t+n+1})}_{\approx Q^{\pi}} - \underbrace{V_{\phi}(s_t)}_{\approx V^{\pi}}
\end{equation*}

Choice of $n$ is a bias-variance trade-off:
%
\begin{align*}
n&=0 && \text{High bias, low variance} \\
n&=\infty && \text{Low bias, high variance}
\end{align*}

Generalized Advantage Estimates\footnote{Schulman et al, 2017: ``High-Dimensional Continuous Control Using Generalized Advantage Estimation"}:
%
\begin{align*}
\hat{A}^{\pi, \lambda}_t &= (1 - \lambda) \sum_{n=0}^{\infty} \lambda^n \hat{A}_t^{\pi(n)}  \\
&= \sum_{t'=t}^{\infty} (\gamma \lambda)^{t'-t} \left(r_{t'} + \gamma V_{\phi}(s_{t'+1}) - V_{\phi}(s_{t'})\right)
\end{align*}

(Those dreaded words: proof left as exercise for the reader)
\end{frame}

\begin{frame}{Vanilla Policy Gradient: Pseudocode}

    \begin{algorithm}[H]
        \caption{Vanilla Policy Gradient Algorithm}
        \label{alg1}
    \begin{algorithmic}[1]
        \STATE Input: initial policy parameters $\theta_0$, initial value function parameters $\phi_0$
        \FOR{$k = 0,1,2,...$} 
        \STATE Collect set of trajectories ${\mathcal D}_k = \{\tau_i\}$ by running policy $\pi_k = \pi(\theta_k)$ in the environment.
        \STATE Compute rewards-to-go $\hat{R}_t$.
        \STATE Compute advantage estimates, $\hat{A}_t$ (using any method of advantage estimation) based on the current value function $V_{\phi_k}$.
        \STATE Estimate policy gradient as
            \begin{equation*}
            \hat{g}_k = \frac{1}{|{\mathcal D}_k|} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T \left. \nabla_{\theta} \log\pi_{\theta}(a_t|s_t)\right|_{\theta_k} \hat{A}_t.
            \end{equation*}
        \STATE Compute policy update, either using standard gradient ascent,
            \begin{equation*}
            \theta_{k+1} = \theta_k + \alpha_k \hat{g}_k,
            \end{equation*}
            or via another gradient ascent algorithm like Adam.
        \STATE Fit value function by regression on mean-squared error:
            \begin{equation*}
            \phi_{k+1} = \arg \min_{\phi} \frac{1}{|{\mathcal D}_k| T} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T\left( V_{\phi} (s_t) - \hat{R}_t \right)^2,
            \end{equation*}
            typically via some gradient descent algorithm.
        \ENDFOR
    \end{algorithmic}
    \end{algorithm}

\end{frame}

\begin{frame}{Trust Region Policy Optimization: Pseudocode}

\begin{algorithm}[H]
   \caption{Trust Region Policy Optimization}
   \label{alg1}
\begin{algorithmic}
     \STATE Input: initial policy parameters $\theta_0$
	 \FOR{$k = 0,1,2,...$} 
	 \STATE Collect set of trajectories $\calD_k$ on policy $\pi_k = \pi(\theta_k)$
	 \STATE Estimate advantages $\hat{A}^{\pi_k}_t$ using any advantage estimation algorithm
	 \STATE Form sample estimates for
	 \begin{itemize}
	 \item policy gradient $\hat{g}_k$ (using advantage estimates)
	 \item and KL-divergence Hessian-vector product function $f(v) = \hat{H}_k v$
	 \end{itemize}
	 \STATE Use CG with $n_{cg}$ iterations to obtain $x_k \approx \hat{H}^{-1}_k \hat{g}_k$
	 \STATE Estimate proposed step $\Delta_k \approx \sqrt{\frac{2\delta}{x_k^T \hat{H}_k x_k}} x_k$
	 \STATE Perform backtracking line search with exponential decay to obtain final update
	 \begin{equation*}
	 \theta_{k+1} = \theta_k + \alpha^j \Delta_k
	 \end{equation*}

	\ENDFOR
\end{algorithmic}
\end{algorithm}

\end{frame}

\begin{frame}{Model-Free RL: Q-Learning}
\begin{figure}
\centering
\includegraphics[width=\linewidth]{rl_algorithms_qlearn_only}
\end{figure}
\end{frame}


\begin{frame}{Model-Free RL: Q-Learning}


\begin{columns}
\begin{column}{0.33\textwidth}
\includegraphics[width=0.9\textwidth]{rl_loop}
\end{column}
\begin{column}{0.67\textwidth}
\textbf{Q-Learning}
\begin{itemize}
\item Run policy: Step in env with action from $Q_{\theta}$, store to replay buffer
\item Evaluate policy: Update $Q_{\theta}$ to minimize Bellman error, using all previous data (\textbf{off-policy})
\item Improve policy: $a^* = \arg \max_{a} Q_{\theta}(s,a)$
\end{itemize}
\end{column}
\end{columns}
\end{frame}


\begin{frame}{Mathematical Foundations of Q-Learning}



\end{frame}

\begin{frame}{Policy Gradients}


\begin{itemize}
\item An algorithm for training stochastic policies:
\begin{itemize}
\item Run current policy in the environment to collect rollouts
\item Take stochastic gradient ascent on policy performance using the \textbf{policy gradient}:
\begin{align*}
g &= \nabla_{\theta} \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T r_t} \\
&= \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \left(\sum_{t'=t}^T r_{t'}\right)}\\
&\approx \frac{1}{|\calD|}\sum_{\tau \in \calD} \sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \left(\sum_{t'=t}^T r_{t'}\right)
\end{align*}
\end{itemize}
\item Core idea: push up the probabilities of good actions and push down the probabilities of bad actions
\item Definition: sum of rewards after time $t$ is the \textit{reward-to-go} at time $t$:
%
\begin{equation*}
\hat{R}_t = \sum_{t'=t}^T r_{t'}
\end{equation*}
\end{itemize}

\end{frame}

\begin{frame}[fragile]{Example Implementation}

\lstset{style=mystyle}
Make the model, loss function, and optimizer:
\begin{lstlisting}[language=python]
 # make model
 with tf.variable_scope('model'):
     obs_ph = tf.placeholder(shape=(None, obs_dim), dtype=tf.float32)
     net = mlp(obs_ph, hidden_sizes=[hidden_dim]*n_layers)
     logits = tf.layers.dense(net, units=n_acts, activation=None)
     actions = tf.squeeze(tf.multinomial(logits=logits,num_samples=1), axis=1)

 # make loss
 adv_ph = tf.placeholder(shape=(None,), dtype=tf.float32)
 act_ph = tf.placeholder(shape=(None,), dtype=tf.int32)
 action_one_hots = tf.one_hot(act_ph, n_acts)
 log_probs = tf.reduce_sum(action_one_hots * tf.nn.log_softmax(logits), axis=1)
 loss = -tf.reduce_mean(adv_ph * log_probs)

 # make train op
 train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)

 sess = tf.InteractiveSession()
 sess.run(tf.global_variables_initializer())
\end{lstlisting}

\end{frame}

\begin{frame}[fragile]{Example Implementation (Continued)}

\lstset{style=mystyle}
One iteration of training:
\begin{lstlisting}[language=python]
# train model for one iteration
batch_obs, batch_acts, batch_rtgs, batch_rets, batch_lens = [], [], [], [], []
obs, rew, done, ep_rews = env.reset(), 0, False, []
while True:
    batch_obs.append(obs.copy())
    act = sess.run(actions, {obs_ph: obs.reshape(1,-1)})[0]
    obs, rew, done, _ = env.step(act)
    batch_acts.append(act)
    ep_rews.append(rew)
    if done:
        batch_rets.append(sum(ep_rews))
        batch_lens.append(len(ep_rews))
        batch_rtgs += list(discount_cumsum(ep_rews, gamma))
        obs, rew, done, ep_rews = env.reset(), 0, False, []
        if len(batch_obs) > batch_size:
            break

# normalize advs trick:
batch_advs = np.array(batch_rtgs)
batch_advs = (batch_advs - np.mean(batch_advs))/(np.std(batch_advs) + 1e-8)
batch_loss, _ = sess.run([loss,train_op], feed_dict={obs_ph: np.array(batch_obs),
                                                     act_ph: np.array(batch_acts),
                                                     adv_ph: batch_advs})
\end{lstlisting}


\lstset{style=mystyle3}

\end{frame}

\begin{frame}{Q-Learning}

\begin{itemize}
\item Core idea: learn $Q^*$ and use it to get the optimal actions
\item Way to do it:
\begin{itemize}
\item Collect experience in the environment using a policy which trades off between acting randomly and acting according to current $Q_{\theta}$
\item Interleave data collection with updates to $Q_{\theta}$ to minimize Bellman error:
%
\begin{equation*}
\min_{\theta} \sum_{(s,a,s',r)\in \calD} \left(Q_{\theta}(s,a) - \left(r + \gamma \max_{a'} Q_{\theta}(s',a') \right) \right)^2
\end{equation*}
...sort of! This actually won't work!
\end{itemize}

\end{itemize}

\end{frame}

\begin{frame}{Getting Q-Learning to Work (DQN)}

Experience replay:
\begin{itemize}
\item Data distribution changes over time: as your $Q$ function gets better and you \textit{exploit} this, you visit different $(s,a,s',r)$ transitions than you did earlier
\item Stabilize learning by keeping old transitions in a replay buffer, and taking minibatch gradient descent on mix of old and new transitions
\end{itemize}
\pause
Target networks:
\begin{itemize}
\item Minimizing Bellman error directly is unstable! 
\item It's \textit{like} regression, but it's not:
%
\begin{equation*}
\min_{\theta} \sum_{(s,a,s',r)\in \calD} \left(Q_{\theta}(s,a) - y(s',r) \right)^2,
\end{equation*}
%
where the target $y(s',r)$ is
%
\begin{equation*}
y(s',r) = r + \gamma \max_{a'} Q_{\theta}(s',a').
\end{equation*}
%
Targets depend on parameters $\theta$---so an update to $Q$ changes the target!
\item Stabilize it by \textit{holding the target fixed} for a while: keep a separate target network, $Q_{\theta_{targ}}$, and every $k$ steps update $\theta_{targ} \leftarrow \theta$
\end{itemize}

\end{frame}

\begin{frame}{DQN Pseudocode}

\begin{algorithm}[H]
\small
   \caption{Deep Q-Learning}
   \label{alg1}
\begin{algorithmic}
     \STATE Randomly generate $Q$-function parameters $\theta$
     \STATE Set target $Q$-network parameters $\theta_{targ} \leftarrow \theta$
     \STATE Make empty replay buffer $\calD$
	 \STATE Receive observation $s_0$ from environment
	 \FOR{$t = 0,1,2,...$} 
	 \STATE With probability $\epsilon$, select random action $a_t$; otherwise select $a_t = \arg \max_{a} Q_{\theta}(s_t, a)$
	 \STATE Step environment to get $s_{t+1}, r_t$ and end-of-episode signal $d_t$
	 \STATE Linearly decay $\epsilon$ until it reaches final value $\epsilon_f$
	 \STATE Store $(s_t, a_t, r_t, s_{t+1}, d_t) \to \calD$
	 \STATE Sample mini-batch of transitions $B = \{(s,a,r,s',d)_i\}$ from $\calD$
	 \STATE For each transition in $B$, compute 
	 \begin{equation*}
	 y = \left\{ \begin{array}{ll}
	 r & \text{transition is terminal }(d=\text{True}) \\
	 r + \gamma \max_{a'} Q_{\theta_{targ}}(s', a') & \text{otherwise}
	 \end{array}\right.
	 \end{equation*}
	 \STATE Update $Q$ by gradient descent on regression loss:
	 %
	 \begin{equation*}
	 \theta \leftarrow \theta - \alpha \nabla_{\theta} \sum_{(s,a,y)\in B} \left(Q_{\theta}(s,a) - y \right)^2
	 \end{equation*}
	 \IF{ $t \% t_{update} =0$}
	 	\STATE Set $\theta_{targ} \leftarrow \theta$
	 \ENDIF

	\ENDFOR
\end{algorithmic}
\end{algorithm}

\end{frame}

\end{document}