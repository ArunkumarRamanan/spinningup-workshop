\documentclass[9pt]{beamer}
\usepackage{beamerthemesplit}
%\usepackage{times}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{thmtools,thm-restate}
\usepackage[font=small]{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{animate}
\usepackage{media9}
\usepackage{multimedia}
\usepackage{hyperref}
\usepackage{cancel}

\usetheme{Madrid}

\graphicspath{{./images/}}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}


\newcommand{\underE}[2]{\underset{\begin{subarray}{c}#1 \end{subarray}}{\E}\left[ #2 \right]}


\newcommand\Fontvi{\fontsize{6}{7.2}\selectfont}
\renewcommand{\d}[1]{\ensuremath{\operatorname{d}\!{#1}}}

\newcommand{\twocolumns}[4]{
\begin{columns}
\begin{column}{#1\textwidth}
    #3
\end{column}
\begin{column}{#2\textwidth}
	#4
\end{column}
\end{columns}
}

\usepackage{listings}
\usepackage{color}
\usepackage{courier}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}


\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize\ttfamily,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    %numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstdefinestyle{mystyle2}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\tiny\ttfamily,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    %numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstdefinestyle{mystyle3}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\scriptsize\ttfamily,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    %numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle3}


\begin{document}
\input{exercises_def.tex}

\title{Introduction to Deep RL, Part 1}
\author{Joshua Achiam}
\institute{OpenAI}
\titlegraphic{\includegraphics[height=2cm]{spinning-up-logo2}}
%\date{March 3, 2018}

\begin{frame}
\titlepage
\end{frame}


\section{1.0: What is Spinning Up?}

\input{1-0_spinning_up.tex}

\section{1.1: What is deep RL, and why do we need it?}

\input{1-1_why_deep_rl.tex}

\section{Interlude: Recap of deep learning patterns}

\input{1-1-5_deep_learning.tex}

\section{1.2: How do we formulate RL problems?}

\input{1-2_formulate_rl.tex}

\section{1.3: What kinds of RL algorithms are there?}

\begin{frame}{Deep RL Algorithms}
There are many different kinds of RL algorithms! This is a non-exhaustive taxonomy (with specific algorithms in blue):
\begin{figure}
\centering
\includegraphics[width=\linewidth]{rl_algorithms_9_15}
\end{figure}
\end{frame}

\begin{frame}{Alphabet soup aside---what are they doing?}


\begin{columns}
\begin{column}{0.33\textwidth}
\includegraphics[width=0.9\textwidth]{rl_loop}
\end{column}
\begin{column}{0.67\textwidth}
Key steps in all RL algorithms:
\begin{itemize}
\item Run policy: actually act in env
\item Evaluate policy: estimate $V^{\pi}$ or $Q^*$
\item Improve policy: do something which lets you pick better actions
\end{itemize}

Major design decisions:
\begin{itemize}
\item Use a model of the env or not (can slot into any of those three steps)
\item Optimize stochastic policy directly or learn $Q^*$ as main controller
\end{itemize}
\end{column}
\end{columns}

\end{frame}

\begin{frame}{Model-Free RL: Policy Optimization}
\begin{figure}
\centering
\includegraphics[width=\linewidth]{rl_algorithms_polopt_only}
\end{figure}
\end{frame}

\begin{frame}{Model-Free RL: Policy Optimization}


\begin{columns}
\begin{column}{0.33\textwidth}
\includegraphics[width=0.9\textwidth]{rl_loop}
\end{column}
\begin{column}{0.67\textwidth}
\textbf{Policy Optimization}
\begin{itemize}
\item Run policy: Collect trajectories $\tau \sim \pi_{\theta}$
\item Evaluate policy: Estimate $V^{\pi_{\theta}}$, $A^{\pi_{\theta}}$ using current trajectories (\textbf{on-policy})
\item Improve policy: Increase likelihood of actions with high advantage
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Warning: Math}

\begin{figure}
\centering
\includegraphics[width=4cm]{palpatine}
\end{figure}

\end{frame}

\begin{frame}{Wait, but why?}


\textbf{Deep RL is not mature enough to be a black box yet: you need to know some of this stuff to work with it successfully.}

\end{frame}

\begin{frame}{Mathematical Foundations of Policy Optimization}

In policy optimization, we train a \textbf{stochastic policy} $\pi_{\theta}$, usually in an \textbf{on-policy way}, to \textbf{maximize performance} $J(\pi_{\theta})$.

\vspace{1em}

What we'll cover:
\begin{itemize}
\item Direct gradient ascent on $J(\pi_{\theta})$ (VPG)
\item Various formulations of $\nabla_{\theta} J(\pi_{\theta})$
\item Some material about natural policy gradients
\end{itemize}

\end{frame}

\begin{frame}{The Policy Gradient}

Goal: derive an expression for $\nabla_{\theta} J(\pi_{\theta})$ which we can compute with a sample estimate, as the basis for a direct gradient ascent algorithm
%
\begin{equation*}
\theta \leftarrow \theta + \alpha \nabla_{\theta} J(\pi_{\theta})
\end{equation*}

\pause
\vspace{1em}

Well what happens if we just...
%
\begin{align*}
\nabla_{\theta} J(\pi_{\theta}) &= \nabla_{\theta} \underE{\tau \sim \pi_{\theta}}{R(\tau)}
\end{align*}

Problem: parameters are in distribution!

\end{frame}

\begin{frame}{The Policy Gradient: First Steps}

Solution: expand expectation into integral, use log-derivative trick
%
\begin{align*}
\nabla_{\theta} J(\pi_{\theta}) &= \nabla_{\theta} \underE{\tau \sim \pi_{\theta}}{R(\tau)} \\
&= \nabla_{\theta} \int d\tau P(\tau | \pi_{\theta}) R(\tau) \\
&= \int d\tau \nabla_{\theta} P(\tau | \pi_{\theta}) R(\tau) \\
&= \int d\tau {\color{red} P(\tau | \pi_{\theta}) \nabla_{\theta} \log P(\tau | \pi_{\theta})} R(\tau) \\
&= \underE{\tau \sim \pi_{\theta}}{\nabla_{\theta} \log P(\tau | \pi_{\theta}) R(\tau) }
\end{align*}

But are we done yet? No! Still need to compute $\nabla_{\theta} \log P(\tau | \pi_{\theta})$

\end{frame}

\begin{frame}{The Policy Gradient: Gradient of Trajectory Distribution}

What is $P(\tau|\pi_{\theta})$?
%
\begin{equation*}
P(\tau | \pi_{\theta}) = \mu(s_0) \prod_{t=0}^T P(s_{t+1} | s_t, a_t) \pi_{\theta}(a_t |s_t)
\end{equation*}

Thus:
%
\begin{align*}
\nabla_{\theta} \log P(\tau|\pi_{\theta}) &= \nabla_{\theta} \log \left(\mu(s_0) \prod_{t=0}^T P(s_{t+1} | s_t, a_t) \pi_{\theta}(a_t |s_t)\right) \\
&= \nabla_{\theta} \left(\log \mu(s_0) + \sum_{t=0}^T \left(\log P(s_{t+1} | s_t, a_t) + \log \pi_{\theta}(a_t |s_t) \right)\right) \\
&= \nabla_{\theta} \log \mu(s_0) + \sum_{t=0}^T \left(\nabla_{\theta} \log P(s_{t+1} | s_t, a_t) + \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \right) \\
&= {\color{red}\cancel{\nabla_{\theta} \log \mu(s_0)}} + \sum_{t=0}^T \left({\color{red}\cancel{\nabla_{\theta} \log P(s_{t+1} | s_t, a_t)}} + \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \right) \\
\therefore \nabla_{\theta} \log P(\tau|\pi_{\theta}) &= \sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t)
\end{align*}

\end{frame}

\begin{frame}{The Policy Gradient: So Far}
Putting it all together so far:
%
\begin{equation*}
\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) R(\tau)}
\end{equation*}

So we could estimate with:
%
\begin{align*}
\nabla_{\theta} J(\pi_{\theta}) &\approx \frac{1}{|\calD|} \sum_{\tau \in\calD} \sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) R(\tau)
\end{align*}

But not good enough! Variance will be high.

\end{frame}

\begin{frame}{The Policy Gradient: Variance Reduction}

Insight: future actions and past rewards should be uncorrelated. That is:
%
\begin{equation*}
\text{for } t > t', \;\;\; \E\left[\nabla_{\theta} \log \pi_{\theta}(a_t | s_t) r_{t'}\right] = 0
\end{equation*}

Thus:
%
\begin{align*}
\nabla_{\theta} J(\pi_{\theta}) &= \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) R(\tau)} \\
&= \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \sum_{t'=0}^T r_{t'}} \\
&= \sum_{t=0}^T \sum_{t'=0}^T \underE{\tau \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t | s_t)r_{t'}} \\
&= \sum_{t=0}^T \sum_{{\color{red}t'=t}}^T\underE{\tau \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t | s_t)r_{t'}} \\
\therefore \nabla_{\theta} J(\pi_{\theta}) &= \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \sum_{t'=t}^T r_{t'}} 
\end{align*}

\end{frame}

\begin{frame}{The Policy Gradient: Alternate Forms}

What we have currently: ``Reward-to-Go" policy gradient:
%
\begin{equation*}
\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \sum_{t'=t}^T r_{t'}} 
\end{equation*}

Observe: expectation can be broken up, letting us transform ``Reward-to-Go" into $Q^{\pi}$:
%
\begin{align*}
\nabla_{\theta} J(\pi_{\theta}) &= \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \sum_{t'=t}^T r_{t'}} \\
&= \sum_{t=0}^T \underE{\tau \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \sum_{t'=t}^T r_{t'}} \\
&= \sum_{t=0}^T \underE{\tau_{0:t} \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \underE{\tau_{(t+1):T} \sim \pi_{\theta}}{\sum_{t'=t}^T r_{t'}}} \\
&= \sum_{t=0}^T \underE{\tau_{0:t} \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t | s_t) Q^{\pi_{\theta}}(s_t, a_t)} \\
\therefore \nabla_{\theta} J(\pi_{\theta}) &=\underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) Q^{\pi_{\theta}}(s_t, a_t)}
\end{align*}

\end{frame}

\begin{frame}{The Policy Gradient: Baselines}

What is a \textbf{baseline}? A function $b(s_t)$ with
%
\begin{equation*}
\nabla_{\theta} J(\pi_{\theta}) =\underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \left(Q^{\pi_{\theta}}(s_t, a_t) - b(s_t)\right)}
\end{equation*}
%
Claim: this works for any $b$! Proof:
%
\begin{align*}
\underE{a_t \sim \pi_{\theta}(\cdot|s_t)}{\nabla_{\theta} \log \pi_{\theta}(a_t | s_t) b(s_t)} &= \underE{a_t \sim \pi_{\theta}(\cdot|s_t)}{\nabla_{\theta} \log \pi_{\theta}(a_t | s_t)} b(s_t) \\
&= \left(\int da \pi_{\theta}(a_t|s_t) \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)\right) b(s_t) \\
&=  \left(\int da {\color{red} \nabla_{\theta} \pi_{\theta}(a_t|s_t) }\right) b(s_t) \\
&= \left({\color{red} \nabla_{\theta}}\int da \pi_{\theta}(a_t|s_t) \right) b(s_t) \\
&=\cancel{\left(\nabla_{\theta} 1 \right)} b(s_t) \\
&= 0
\end{align*}

\end{frame}

\begin{frame}{The Policy Gradient: Advantage Form}
If we choose $b = V^{\pi}$, we get the \textbf{advantage form} of the policy gradient:
%
\begin{align*}
\nabla_{\theta} J(\pi_{\theta}) &=\underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \left(Q^{\pi_{\theta}}(s_t, a_t) - V^{\pi_{\theta}}(s_t)\right)}\\
 &=\underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) A^{\pi_{\theta}}(s_t, a_t)}
\end{align*}

Why do we want this? Better signal in sample estimate: removes ``stuff that would have happened anyway" from $Q^{\pi}$
\end{frame}

\begin{frame}{The Policy Gradient: Review}

What we've shown so far:
%
\begin{align*}
\nabla_{\theta} J(\pi_{\theta}) &=\underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) R(\tau)}\\ 
&=\underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \sum_{t'=t}^T r_{t'}}\\
&=\underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) Q^{\pi_{\theta}}(s_t, a_t)}\\
 &=\underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) A^{\pi_{\theta}}(s_t, a_t)}
\end{align*}

Which form do we use? Almost always the last one.

\end{frame}

\begin{frame}{The Policy Gradient: Key Concepts}

\begin{equation*}
\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^T \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) A^{\pi_{\theta}}(s_t, a_t)}
\end{equation*}

\begin{itemize}
\item Pushes up the probabilities of ``good" actions and pushes down probabilities of ``bad" actions
\item To estimate, \textbf{must sample on-policy}
\end{itemize}
\end{frame}

\begin{frame}{The Policy Gradient: Policy Evaluation}

The policy gradient expression gives us the \textbf{policy improvement} step, but we need to compute advantages: so how do we do \textbf{policy evaluation}?

\vspace{1em}

Answer: first learn $V^{\pi}$ approximator by regression:
%
\begin{equation*}
\min_{\phi} \frac{1}{N}\sum_{\tau \in \calD} \sum_{t=0}^T \left(V_{\phi}(s_t) - \sum_{t'=t}^T \gamma^{t'-t} r_{t'}\right)^2
\end{equation*}

Then use it to estimate $A^{\pi}$, usually with \textbf{generalized advantage estimation}

\pause
\vspace{2em}

Wait, did you pull a fast one? Discount factor is in now? \textbf{Policy gradient implementations usually use discounted value functions, even though they (ostensibly) optimize the undiscounted objective.}
\end{frame}

\begin{frame}{The Policy Gradient: Generalized Advantage Estimation}

N-Step Advantage Estimates:
%
\begin{equation*}
\hat{A}_t^{\pi(n)} = \underbrace{\sum_{t'=t}^n \gamma^{t'-t} r_{t'} + \gamma^{n+1} V_{\phi}(s_{t+n+1})}_{\approx Q^{\pi}} - \underbrace{V_{\phi}(s_t)}_{\approx V^{\pi}}
\end{equation*}

Choice of $n$ is a bias-variance trade-off:
%
\begin{align*}
n&=0 && \text{High bias, low variance} \\
n&=\infty && \text{Low bias, high variance}
\end{align*}

Generalized Advantage Estimates\footnote{Schulman et al, 2017: ``High-Dimensional Continuous Control Using Generalized Advantage Estimation"}:
%
\begin{align*}
\hat{A}^{\pi, \lambda}_t &= (1 - \lambda) \sum_{n=0}^{\infty} \lambda^n \hat{A}_t^{\pi(n)}  \\
&= \sum_{t'=t}^{\infty} (\gamma \lambda)^{t'-t} \left(r_{t'} + \gamma V_{\phi}(s_{t'+1}) - V_{\phi}(s_{t'})\right)
\end{align*}

(Those dreaded words: proof left as exercise for the reader)
\end{frame}

\begin{frame}{Vanilla Policy Gradient: Pseudocode}
    \begin{algorithm}[H]
        \caption{Vanilla Policy Gradient Algorithm}
        \label{alg1}
    \begin{algorithmic}[1]
    \footnotesize{
        \STATE Input: initial policy parameters $\theta_0$, initial value function parameters $\phi_0$
        \FOR{$k = 0,1,2,...$} 
        \STATE Collect set of trajectories ${\mathcal D}_k = \{\tau_i\}$ by running policy $\pi_k = \pi(\theta_k)$ in the environment.
        \STATE Compute rewards-to-go $\hat{R}_t$.
        \STATE Compute advantage estimates, $\hat{A}_t$ (using any method of advantage estimation) based on the current value function $V_{\phi_k}$.
        \STATE Estimate policy gradient as
            \begin{equation*}
            \hat{g}_k = \frac{1}{|{\mathcal D}_k|} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T \left. \nabla_{\theta} \log\pi_{\theta}(a_t|s_t)\right|_{\theta_k} \hat{A}_t.
            \end{equation*}
        \STATE Compute policy update, either using standard gradient ascent,
            \begin{equation*}
            \theta_{k+1} = \theta_k + \alpha_k \hat{g}_k,
            \end{equation*}
            or via another gradient ascent algorithm like Adam.
        \STATE Fit value function by regression on mean-squared error:
            \begin{equation*}
            \phi_{k+1} = \arg \min_{\phi} \frac{1}{|{\mathcal D}_k| T} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T\left( V_{\phi} (s_t) - \hat{R}_t \right)^2,
            \end{equation*}
            typically via some gradient descent algorithm.
        \ENDFOR
        }
    \end{algorithmic}
    \end{algorithm}
\end{frame}

\begin{frame}{Trust Region Policy Optimization: Pseudocode}

\begin{algorithm}[H]
   \caption{Trust Region Policy Optimization}
   \label{alg1}
\begin{algorithmic}
     \STATE Input: initial policy parameters $\theta_0$
	 \FOR{$k = 0,1,2,...$} 
	 \STATE Collect set of trajectories $\calD_k$ on policy $\pi_k = \pi(\theta_k)$
	 \STATE Estimate advantages $\hat{A}^{\pi_k}_t$ using any advantage estimation algorithm
	 \STATE Form sample estimates for
	 \begin{itemize}
	 \item policy gradient $\hat{g}_k$ (using advantage estimates)
	 \item and KL-divergence Hessian-vector product function $f(v) = \hat{H}_k v$
	 \end{itemize}
	 \STATE Use CG with $n_{cg}$ iterations to obtain $x_k \approx \hat{H}^{-1}_k \hat{g}_k$
	 \STATE Estimate proposed step $\Delta_k \approx \sqrt{\frac{2\delta}{x_k^T \hat{H}_k x_k}} x_k$
	 \STATE Perform backtracking line search with exponential decay to obtain final update
	 \begin{equation*}
	 \theta_{k+1} = \theta_k + \alpha^j \Delta_k
	 \end{equation*}

	\ENDFOR
\end{algorithmic}
\end{algorithm}

\end{frame}

\begin{frame}{Model-Free RL: Q-Learning}
\begin{figure}
\centering
\includegraphics[width=\linewidth]{rl_algorithms_qlearn_only}
\end{figure}
\end{frame}


\begin{frame}{Model-Free RL: Q-Learning}


\begin{columns}
\begin{column}{0.33\textwidth}
\includegraphics[width=0.9\textwidth]{rl_loop}
\end{column}
\begin{column}{0.67\textwidth}
\textbf{Q-Learning}
\begin{itemize}
\item Run policy: Step in env with action from $Q_{\theta}$, store to replay buffer
\item Evaluate policy: Update $Q_{\theta}$ to minimize Bellman error, using all previous data (\textbf{off-policy})
\item Improve policy: $a^* = \arg \max_{a} Q_{\theta}(s,a)$
\end{itemize}
\end{column}
\end{columns}
\end{frame}


\begin{frame}{Q-Learning Updates by Bootstrapping}

\begin{itemize}
\item Collect experience in the environment using a policy which trades off between acting randomly and acting according to current $Q_{\theta}$
\item Interleave data collection with updates to $Q_{\theta}$ to minimize Bellman error by bootstrapping:
%
\begin{equation*}
\min_{\theta} \sum_{(s,a,s',r)\in \calD} \left(Q_{\theta}(s,a) - y(s',r) \right)^2
\end{equation*}
%
where
\begin{equation*}
y(s',r) = r + \gamma \max_{a'} Q_{\theta}(s',a'),
\end{equation*}
%
and gradients don't propagate through $y$.
\end{itemize}


\end{frame}

\begin{frame}{Getting Q-Learning to Work (DQN)}

\textbf{Experience replay}:
\begin{itemize}
\item Data distribution changes over time: as your $Q$ function gets better and you \textit{exploit} this, you visit different $(s,a,s',r)$ transitions than you did earlier
\item Stabilize learning by keeping old transitions in a replay buffer, and taking minibatch gradient descent on mix of old and new transitions
\end{itemize}
\pause
\textbf{Target networks}:
\begin{itemize}
\item Bootstrapping with function approximators is unstable! 
\item It's \textit{like} regression, but it's not: targets depend on parameters $\theta$---so an update to $Q$ changes the target!
%
\begin{equation*}
y(s',r) = r + \gamma \max_{a'} Q_{\theta}(s',a').
\end{equation*}
%
\item Stabilize it by \textit{holding the target fixed} for a while: keep a separate target network, $Q_{\theta_{targ}}$, and every $k$ steps update $\theta_{targ} \leftarrow \theta$
\item If unstable, why do it? Because it works well! (Plus theory, later)
\end{itemize}

\end{frame}

\begin{frame}{In DQN, Action Space Matters}

What we've described so far requires the computation of
%
\begin{equation*}
\max_{a} Q_{\theta}(s,a),
\end{equation*}
%
both for selecting actions and calculating update targets.

\vspace{1em}
\textbf{Problem}: for continuous action spaces, this is nontrivially hard! DQN therefore only applies for \textbf{discrete actions}, because we can output one Q-value per action from the network.

\begin{figure}
\centering
\includegraphics[width=7cm]{dqn-nature}
\end{figure}

\footnotemark[1]{Image credit: Mnih et al, 2015}
\end{frame}

\begin{frame}{DQN Pseudocode}

\begin{algorithm}[H]
\small
   \caption{Deep Q-Learning}
   \label{alg1}
\begin{algorithmic}
     \STATE Randomly generate $Q$-function parameters $\theta$
     \STATE Set target $Q$-network parameters $\theta_{targ} \leftarrow \theta$
     \STATE Make empty replay buffer $\calD$
	 \STATE Receive observation $s_0$ from environment
	 \FOR{$t = 0,1,2,...$} 
	 \STATE With probability $\epsilon$, select random action $a_t$; otherwise select $a_t = \arg \max_{a} Q_{\theta}(s_t, a)$
	 \STATE Step environment to get $s_{t+1}, r_t$ and end-of-episode signal $d_t$
	 \STATE Linearly decay $\epsilon$ until it reaches final value $\epsilon_f$
	 \STATE Store $(s_t, a_t, r_t, s_{t+1}, d_t) \to \calD$
	 \STATE Sample mini-batch of transitions $B = \{(s,a,r,s',d)_i\}$ from $\calD$
	 \STATE For each transition in $B$, compute 
	 \begin{equation*}
	 y = \left\{ \begin{array}{ll}
	 r & \text{transition is terminal }(d=\text{True}) \\
	 r + \gamma \max_{a'} Q_{\theta_{targ}}(s', a') & \text{otherwise}
	 \end{array}\right.
	 \end{equation*}
	 \STATE Update $Q$ by gradient descent on regression loss:
	 %
	 \begin{equation*}
	 \theta \leftarrow \theta - \alpha \nabla_{\theta} \sum_{(s,a,y)\in B} \left(Q_{\theta}(s,a) - y \right)^2
	 \end{equation*}
	 \IF{ $t \% t_{update} =0$}
	 	\STATE Set $\theta_{targ} \leftarrow \theta$
	 \ENDIF

	\ENDFOR
\end{algorithmic}
\end{algorithm}

\end{frame}

\begin{frame}{Caveat Emptor: DQN Can Easily Break}

Even with many additional tricks, DQN-style algorithms still occasionally diverge (Q-values go to unrealistically large or negative values and control performance tanks). Why?

\vspace{2em}
\textbf{To answer that question, we have to dive into some math.}

\begin{figure}
\centering
\includegraphics[width=7cm]{q_divergence}
\end{figure}

\footnotemark[1]{van Hesselt et al, 2018: ``Deep Reinforcement Learning and the Deadly Triad"}

\end{frame}

\begin{frame}{Mathematical Foundations of Q-Learning}

Operator view of Bellman equation: define \textbf{optimal Bellman operator} $\calT^* : \calQ \to \calQ$ to have values
%
\begin{equation*}
[\calT^*Q](s,a) = \underE{s' \sim P}{R(s,a,s') + \gamma \max_{a'} Q(s',a')}.
\end{equation*}

\vspace{2em}
Optimal Q-function is fixed-point of $\calT^*$:
%
\begin{equation*}
Q^* = \calT^* Q^*
\end{equation*}

$\calT^*$ has a special property: it is a \textbf{contraction} on $\calQ$ in the sup norm (max absolute value over all states and actions).

\end{frame}

\begin{frame}{Foundations of Q-Learning: Contraction Maps}

A function $f: X \to X$ on a normed vector space $(X, \|\cdot\|)$ is said to be a \textbf{contraction} with modulus $\beta$ if
%
\begin{equation*}
\forall x, y \in X, \;\;\; \|f(x) - f(y)\| \leq \beta \|x - y\|.
\end{equation*}

\vspace{2em}
Why do we care about contractions? Because they have \textbf{unique fixed-points} and the fixed points can be obtained by repeated application of the operator!

\vspace{2em}
To see this, consider the sequence $\{x_0, x_1, ...\}$ with $x_{i+1} = f(x_i)$. Observe that
%
\begin{align*}
\|x_{i+1} - x_i\| &= \|f(x_i) - f(x_{i-1})\|\\
&\leq \beta \|x_{i} - x_{i-1}\| \\
&\leq \beta^i \|x_1 - x_0\|
\end{align*}

The distance between iterates shrinks by a factor of $\beta$ each iteration!

\end{frame}

\begin{frame}{Foundations of Q-Learning: Value Iteration}

$\calT^*$ is a contraction on $\calQ$ with modulus $\gamma$, so we should be able to obtain $Q^*$ in the limit of the sequence:
%
\begin{equation*}
Q_{i+1} = \calT^* Q_i.
\end{equation*}

This approach is \textbf{value iteration}, a classic RL algorithm. 

\vspace{2em}

\textbf{Problems}: When the state/action space is large and we use function approximation, 
%
\begin{itemize}
\item We can't compute all of $\calT^*Q_i$
\item $\calT^* Q_i$ may not be representable in our function class $\calQ_{\theta}$
\end{itemize}

\end{frame}

\begin{frame}{Foundations of Q-Learning: Approximating Value Iteration}

So what we do is push $Q_{\theta}$ towards $\calT^* Q_{\theta}$ for the state-action pairs where we can estimate it:
%
\begin{equation*}
\theta \leftarrow \theta + \alpha \underbrace{\left(\hat{\calT}^*Q_{\theta}(s,a) - Q_{\theta}(s,a)\right)}_{r + \gamma \max_{a'} Q_{\theta}(s',a') - Q_{\theta}(s,a)} \nabla_{\theta} Q_{\theta}(s,a)
\end{equation*}
%
which is deep Q-learning. This may or may not preserve the contraction: when it doesn't, divergence happens!

\vspace{2em}

Important to realize: \textbf{this means deep Q-learning isn't minimizing a real loss function}.

\end{frame}

\begin{frame}{Model-Based RL}
\begin{figure}
\centering
\includegraphics[width=\linewidth]{rl_algorithms_modelbased_only}
\end{figure}
\end{frame}

\begin{frame}{Model-Based RL}


\begin{columns}
\begin{column}{0.33\textwidth}
\includegraphics[width=0.9\textwidth]{rl_loop}
\end{column}
\begin{column}{0.67\textwidth}
\textbf{Model-Based RL}
\begin{itemize}
\item Can make use of an environment model (a simulator) anywhere in the loop to improve!
\item Downside: model not usually available
\item Models can be very hard to learn, so model-based is not yet as reliable as model-free
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\end{document}